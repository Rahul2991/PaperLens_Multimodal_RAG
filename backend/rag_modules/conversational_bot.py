import ollama
import logging

# Configure logger
logger = logging.getLogger("Multimodal_rag_bot")

class Conversational_Bot:
    """
    A conversational AI chatbot that interacts with users using a language model.
    
    Attributes:
        messages (list): Stores the chat history.
    """
    def __init__(self, system=""):
        """
        Initializes the chatbot with an optional system instruction.

        Args:
            system (str, optional): System-level instruction for the chatbot.
        """
        self.messages = [] # define history list
        
        if system:
            logger.info("Initializing bot with system instructions.")
            self.messages.append({"role": "system", "content": system})
            
    def generate(self, user_question, image=None):
        """
        Generates a response from the language model based on user input.

        Args:
            user_question (str): The user's query.
            image (str, optional): Image input for multimodal processing.

        Returns:
            dict: Response generated by the language model.
        """
        logger.info("Generating response for user query.")
    
        # Append user query to history under the "user" role
        if image:
            self.messages.append({"role": "user", "content":user_question, "images": [image]})
            logger.info("User query includes an image.")
        else:
            self.messages.append({"role": "user", "content":user_question})
                
        # Generate response from the language model
        response = ollama.chat(model='llama3.2-vision', messages=self.messages)
        
        # Add LLM's response to the history under "assistant" role
        self.messages.append({"role":"assistant", "content":response.message.content})
        
        print(f'History: {self.messages}')
        
        return response
    
    def get_history(self):
        """
        Retrieves the chat history.

        Returns:
            list: Chat history.
        """
        logger.info("Fetching chat history.")
        return self.messages
    
    def set_history(self, history):
        """
        Sets the chat history to a provided list.

        Args:
            history (list): The chat history to set.
        """
        logger.info("Setting new chat history.")
        self.messages = history
        
    def summarize_image(self, image):
        """
        Generates a textual summary of an image.

        Args:
            image (str): The image data.

        Returns:
            str: Summary of the image.
        """
        logger.info("Generating image summary.")
        
        response = ollama.chat(
        model='llama3.2-vision',
        messages=[{
            'role': 'user',
            'content': 'Summarize the image:',
            'images': [image]
            }]
        )
        return response.message.content
    
    def summarize_table(self, table_html):
        """
        Generates a summary of an HTML table.

        Args:
            table_html (str): The HTML representation of a table.

        Returns:
            str: Summary of the table.
        """
        logger.info("Generating table summary.")
        
        response = ollama.chat(
        model='llama3.2:1b',
        messages=[{
            'role': 'user',
            'content': f'Summarize this table: {table_html}'
            }]
        )
        return response.message.content
    
if __name__ == '__main__':
    bot = Conversational_Bot("You are an expert in the field of AI Research and current AI Trends.")
    query = None
    while query != 'EXIT':
        query = input('You: ')
        if query == 'EXIT': break
        response = bot.generate(query)
        print(f'Bot: {response.message.content}')